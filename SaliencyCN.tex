% XeLaTex
%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

%\usepackage{ctex}
\usepackage{bm}
\usepackage[UTF8]{ctex}
\usepackage{mathrsfs}
%\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{overpic}

\usepackage{enumitem}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}


%\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[breaklinks=true,colorlinks,bookmarks=false]{hyperref}


%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{159} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2020}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\cmm}[1]{\textcolor[rgb]{0,0.6,0}{CMM: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\alert}[1]{\textcolor[rgb]{.6,0,0}{#1}}

\newcommand{\IT}{IT\cite{98pami/Itti}}
\newcommand{\MZ}{MZ\cite{03ACMMM/Ma_Contrast-based}}
\newcommand{\GB}{GB\cite{conf/nips/HarelKP06}}
\newcommand{\SR}{SR\cite{07cvpr/hou_SpectralResidual}}
\newcommand{\FT}{FT\cite{09cvpr/Achanta_FTSaliency}}
\newcommand{\CA}{CA\cite{10cvpr/goferman_context}}
\newcommand{\LC}{LC\cite{06acmmm/ZhaiS_spatiotemporal}}
\newcommand{\AC}{AC\cite{08cvs/achanta_salient}}
\newcommand{\HC}{HC-maps }
\newcommand{\RC}{RC-maps }
\newcommand{\Lab}{$L^*a^*b^*$}
\newcommand{\mypara}[1]{\paragraph{#1.}}

\graphicspath{{figures/}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{409}

\begin{document}
% \begin{CJK*}{GBK}{song}

\renewcommand{\figref}[1]{图\ref{#1}}
\renewcommand{\tabref}[1]{表\ref{#1}}
\renewcommand{\equref}[1]{式\ref{#1}}
\renewcommand{\secref}[1]{第\ref{#1}节}
\def\abstract{\centerline{\large\bf 摘要} \vspace*{12pt} \it}

%%%%%%%%% TITLE

\title{卷积神经网络的最新进展\thanks{本文为论文
\cite{2015Recent}的中文翻译版，译者:郭哲宏。由于内容较多，所以在github上持续更新，地址:https://github.com/vabook/paper}}


\author{Jiuxiang Gu$^{1}$\quad Zhenhua Wang$^{2}$ \quad Jason Kuen$^{2}$ \quad 
	Lianyang Ma$^{2}$ \quad Amir Shahroudy$^{2}$  \quad \\
	Bing Shuai$^{2}$ \quad   Ting Liu$^{2}$	\quad Xingxing Wang$^{2}$
	\quad Li Wang$^{2}$ \quad Gang Wang$^{2}$ \quad \\ Jianfei Cai$^{3}$ \quad  Tsuhan Chen$^{3}$\\
	$^{1}$ ROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore\\
	$^{2}$School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore\\
	$^{3}$ School of Computer Science and Engineering, Nanyang Technological University, Singapore\\
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
在过去的几年中，深度学习在视觉识别、语音识别和自然语言处理等各种问题上都取得了很好的效果。在不同类型的深度神经网络中，卷积神经网络的研究最为广泛。利用注释数据量的快速增长和图形处理器单元性能的巨大改进，卷积神经网络的研究迅速出现，并在各种任务上取得了最先进的结果。在本文中，我们提供了一个广泛的调查，在卷积神经网络方面最近的进展。我们从层设计、激活函数、损失函数、正则化、优化和快速计算等方面详细介绍了CNN的改进。此外，我们还介绍了卷积神经网络在计算机视觉、语音和自然语言处理中的各种应用。
\end{abstract}





%%%%%%%%% BODY TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{引言}\label{sec:Introduction}
卷积神经网络(CNN)是一个著名的深度学习架构，灵感来自生物的自然视觉感知机制。1959年，Hubel \& Wiesel发现动物视觉皮层的细胞负责感受野的光探测。
受此启发，日本福岛邦彦于1980年提出了新认知电子学，可以说是CNN的前身。1990年LeCun等人发表了开创性的论文，建立了CNN的现代框架，并对其进行了改进。他们开发了一种名为LeNet-5的多层人工神经网络，可以对手写数字进行分类。像其他神经网络一样，LeNet-5有多层，可以用反向传播算法进行训练。该方法可以获得原始图像的有效表示，使得无需预处理就能直接从原始像素中识别视觉模式成为可能。张等人的并行研究使用人工神经网络(SIANN)从图像中识别字符。然而，由于当时缺乏大量的训练数据和计算能力，他们的网络不能很好地处理更复杂的问题，例如大规模的图像和视频分类。自2006年以来，已经发展了许多方法来克服训练深度cnn所遇到的困难。最值得注意的是，Krizhevsky等人提出了一个经典的CNN架构，并在图像分类任务上显示了对以前方法的显著改进。他们的方法的整体架构，即AlexNet，与LeNet-5类似，但具有更深层次的结构。随着AlexNet的成功，人们提出了许多改进其性能的工作。其中，有代表性的工作有四个分别是ZFNet, VGGNet, GoogleNet and ResNet。从架构演变来看，一个典型的趋势是网络越来越深，例如2015年ILSVRC冠军的ResNet比AlexNet深度约20倍，比VGGNet深度约8倍。通过增加深度，网络可以更好地逼近非线性增加的目标函数，得到更好的特征表示。但是，它也增加了网络的复杂性，使得网络更难以优化，更容易得到过拟合。在这个过程中，人们从各个方面提出了各种方法来解决这些问题。在本文中，我们试图对最近的进展进行全面的回顾，并进行一些深入的讨论。

在下面的章节中，我们将确定与CNN相关的工作的大致类别。\figref{fig:juanji}显示了本文的层次结构分类。我们首先在第2节中概述CNN的基本组件。然后在第3节介绍了CNN在不同方面的一些最新改进，包括卷积层、池化层、激活函数、损失函数、正则化和优化，并在第4节介绍了快速计算技术。接下来，我们在第5节中讨论了CNN的一些典型应用，包括图像分类、目标检测、目标跟踪、姿态估计、文本检测与识别、视觉显著性检测、动作识别、场景标记、语音和自然语言处理。最后，我们在第6节对本文进行总结。






\begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{1.png}
    \end{overpic}
    \caption{卷积神经网络的层次结构分类
    }\label{fig:juanji}
\end{figure}




\section{CNN基本组件}
\label{sec:RelatedWorks}
在文献中有很多CNN架构的变体。然而，它们的基本成分非常相似。以著名的LeNet-5为例，它由三种类型的层组成，即卷积层、池化层和全连接层。卷积层的目的是学习输入的特征表示。如\figref{fig:LeNet}(a)所示，卷积层由几个卷积核组成，这些卷积核用于计算不同的特征图。具体地说，特征图的每个神经元都连接到前一层邻近神经元的区域。这样的邻域被称为前一层神经元的接受域。新的特征图可以通过先将输入与学习过的核函数卷积，然后在卷积结果上应用一个元素级非线性激活函数得到。注意，要生成每个特性图，输入的所有空间位置都共享内核。通过使用几个不同的内核，得到了完整的特征映射。数学上，第l层的k个特征图$z^l_{i,j,k}$中$(i,j)$位置的特征值是这样计算的:
\begin{equation}
z^l_{i,j,k}=w^l_kx^l_{i,j}+b^l_k
\end{equation}
\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{2.png}
	\end{overpic}
	\caption{LeNet-5网络的结构
	}\label{fig:LeNet}
\end{figure}

式中$w^l_k$和$b^l_k$分别为第l层第k个滤波器的权值向量和偏置项，$x^l_{i,j}$是以第l层$(i, j)$位置为中心的输入块。请注意内核$w^l_k$生成特性映射$z^l_{:,:,k}$是共享的。这种权值共享机制具有降低模型复杂度、使网络更容易训练等优点。激活函数将非线性引入CNN，这是多层网络检测非线性特征的理想方法。设$a(·)$表示非线性激活函数。卷积特征$z^l_{i,j,k}$的激活值$a^l_{i,j,k}$可以计算为:
\begin{equation}
	a^l_{i,j,k}=a(z^l_{i,j.k})
\end{equation}

典型的激活函数是sigmoid、tanh和ReLU。池化层的目的是通过降低特征图的分辨率来实现偏移不变性。它通常被放置在两个卷积层之间。池化层的每个特征图都与前一个卷积层对应的特征图相连接。将池函数表示为$pool(·)$，对于每个特征$a^l_{:,:,k}$都有:
\begin{equation}
	y^l_{i,j,k}=pool(a^l_{m,n,k}),\forall(m,n)\in R_{i,j}
\end{equation}
其中$R_{i,j}$是位置$(i, j)$附近的局部邻域。典型的池化操作是平均池化和最大池化。图2(b)是前两个卷积层学习到的数字7的特征图。第一卷积层的核被设计用于检测边缘和曲线等低级特征，而更高层的核被学习用于编码更抽象的特征。通过叠加几个卷积和池化层，我们可以逐渐提取更高层次的特征表示。

在几个卷积层和池化层之后，可能有一个或多个全连接层，目的是执行高级推理。它们将前一层的所有神经元与当前层的每一个神经元连接起来，生成全局语义信息。请注意，全连接层并不总是必要的，因为它可以被1 × 1卷积层取代。

CNN的最后一层是输出层。对于分类任务，通常使用softmax操作符。另一种常用的方法是SVM，它可以结合CNN的特征来解决不同的分类任务。让\emph{\textbf{$\theta$}}表示CNN的所有参数(例如，权重向量和偏差项)。通过最小化定义在该任务上的适当损失函数，可以获得特定任务的最佳参数。假设我们有N个期望的输入输出关系${(x^{(n)}, y^{(n)}); n\in [1，···，N]}$，其中$x^(n)$为第N个输入数据，$y^(n)$为其对应的目标标签，$o^(n)$为CNN的输出。CNN的损失值可以计算如下:
\begin{equation}
	L = \frac{1}{N}\sum_{n=1}^{N}l(\theta;y^{(n)},o^{(n)})
\end{equation}
训练CNN是一个全局优化问题。通过最小化损失函数，可以找到最佳的参数拟合集。随机梯度下降法是优化CNN网络的常用方法。

\section{CNN的改进}
自从AlexNet在2012年成功以来，cnn已经有了各种各样的改进。在本节中，我们将从六个方面描述cnn的主要改进:卷积层、池化层、激活函数、损失函数、正则化和优化器。

\subsection{卷积层}
基本神经网络中的卷积滤波器是一种广义线性模型(GLM)。当潜在概念的实例是线性可分的时候，它很适合抽象概念。在这里我们介绍一些旨在提高其表现能力的工作。
\subsubsection{平铺卷积(Tiled Convolution)}
神经网络中的权值共享机制可以大大减少参数的数量。然而，它也可能限制模型学习其他种类的不变性。Tiled CNN是CNN的变种，拼贴并且复联特征图来学习旋转和规模不变的特征。在同一层中分别学习不同的核，通过对相邻单元进行平方根池化，可以隐式地学习复不变性。如图3(b)所示，卷积运算在每k个单元中进行，其中k是拼贴的大小，用来控制共享权值的距离。当拼贴的大小k为1时，每个map内的单位将具有相同的权重，tile CNN将与传统CNN相同。在一篇论文中中，人们在NORB和CIF AR-10数据集上的实验表明，k = 2的结果最好。Wang等人发现Tiled CNN在小时间序列数据集上比传统CNN表现更好。
\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{3.png}
	\end{overpic}
	\caption{四种卷积层
	}\label{fig:LeNet}
\end{figure}
\subsubsection{转置卷积(Transposed Convolution)}
转置卷积可以看作是相应的传统卷积的后向传递。它也被称为反卷积和分步卷积。为了与大多数文献一致，我们使用术语“反卷积”。与传统的将多个输入激活连接到单个激活的卷积相反，反卷积将单个激活与多个输出激活相关联。图3(d)显示了在4 × 4输入上使用单位步幅和零填充的3 × 3核的反卷积运算。反卷积的步幅给出了输入特征图的膨胀因子。具体来说，反卷积首先对输入进行填充步长值的一个因子的上采样，然后对上采样的输入进行卷积运算。目前，反卷积被广泛应用于可视化、识别、定位、语义分割、视觉问答、超分辨率等领域。
\subsubsection{扩张卷积(Dilated Convolution)}
Dilated CNN是CNN的最新发展，它为卷积层引入了一个超参数。Dilated CNN通过在过滤器元素之间插入零，可以增加网络接受域的大小，让网络覆盖更多的相关信息。这对于那些在做预测时需要很大接受域的任务来说是非常重要的。形式上，将信号$F$与核$k$大小为r的卷积的一维扩展卷积定义为$(F ∗_l k)_t=\Sigma_{\tau}k_{\tau}F_{t−l\tau}$，其中$∗l$表示$l$扩展卷积。这个公式可以直接推广到二维扩张卷积。图3(c)显示了三个膨胀的卷积层的一个例子，其中膨胀因子$l$在每一层呈指数增长。中间的特征$F_2$由底部的特征图$F_1$通过施加一个扩展卷积产生，其中$F_2$中的每个元素都有一个接受域大小为$3×3$。$F_3$由$F_2$通过施加2扩张的卷积产生，其中$F_3$中的每个元素都有一个$(2^3−1)×(2^3−1)$的接受域。最上面的特征图$F_4$是由$F_3$通过4扩张卷积得到的，其中$F_4$中的每个元素的接受域为$(2^4−1)×(2^4−1)$。可以看出，$F_{i+1}$中每个元素的感受场大小为$(2^{i+2}−1)×(2^(i+2)−1)$。Dilated cnn在场景分割[37]、机器翻译[38]、语音合成[39]和语音识别[40]等任务中取得了令人印象深刻的性能。
\subsubsection{Network in Network}
Network in Network(NIN)是Lin等人提出的一种通用网络结构。它用微网络代替了卷积层的线性滤波器，如本文中的多层感知器卷积(mlpconv)层，使其能够近似于更抽象的潜在概念表示。NIN的整体结构就是这些微网络的堆叠。\figref{fig:compar}显示了线性卷积层和mlpconv层之间的区别。形式上，卷积层的特征图(具有非线性激活函数，如ReLU[14])计算为
\begin{equation}
	a_{i,j,k}=max(x^T_kx_{i,j}+b_k,0)
\end{equation}
\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{4.png}
	\end{overpic}
	\caption{线性卷积层与mlpconv层的比较。
	}\label{fig:compar}
\end{figure}
其中$a_{i,j,k}$是第k个特征图在(i, j)位置的激活值，$x_{i,j}$是以$(i, j)$位置为中心的输入, $w_k$ 和$b_k$ 是第k个滤波器的权重向量和偏置项。作为比较，mlpconv层执行的计算公式为
\begin{equation}
	a^n_{i,j,k_n}=max(w^T_{k_n}a^{n-1}_{i,j,:}+b_{k_n},0)
\end{equation}
其中$n\in[1,N]$,N是mlpconv层的层数，$a^0_{i,j,:}$等于$x_{i,j}$。mlpconv层在传统卷积层之后放置了1×1卷积。1×1卷积相当于ReLU所继承的跨通道参数池操作。因此，mlpconv层也可以看作是正常卷积层上级联的跨通道参数池化。最后，利用全局平均池对最后一层的特征图进行空间平均，并将输出向量直接输入到softmax层。与全连接层相比，全局平均池具有更少的参数，从而降低了过拟合风险和计算负荷。
\subsubsection{Inception Module}
Inception Module由Szegedy等人引入，可以看作是NIN的逻辑顶点。他们使用不同尺寸的过滤器来捕捉不同尺寸的不同视觉模式，近似最优稀疏结构由Inception Module实现。具体而言，inception Module实现由一个池化操作和三种类型的卷积操作组成(见\figref{fig:Inceptionmodule}(b))，在3×3和5×5的卷积之前放置1 × 1 convolutions作为降维模块，这样可以在不增加计算复杂度的情况下增加CNN的深度和宽度。在初始模块的帮助下，网络参数可以大幅降低到500万，远低于AlexNet(6000万)和ZFNet(7500万)
\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{5.png}
	\end{overpic}
	\caption{不同版本的Inception Module
	}\label{fig:Inceptionmodule}
\end{figure}

在他们后来的论文，为了找到具有相对适中计算成本的高性能网络，他们建议表示大小应该从输入到输出缓慢减少，并且在不损失表示能力的情况下，可以在低维嵌入上进行空间聚合。通过平衡每层滤波器的数量和网络的深度，可以达到网络的最佳性能。受ResNet的启发，他们最新的inception-\emph{V4}结合了inception体系结构和快捷连接(见\figref{fig:Inceptionmodule}(d))。他们发现，快捷连接可以显著加快初始网络的训练。在\emph{ILSVRC2012}的验证数据集上，他们集成了3个残差和1个Inception-v4模型体系结构(有75个可训练层)可以达到$3.08\%$的前5错误率。

\subsection{池化层}
池化是CNN的一个重要概念。它通过减少卷积层之间的连接数来降低计算负担。在本节中，我们将介绍最近在CNN中使用的一些的池化方法。
\subsubsection{\emph{$L_P$}池化}
\emph{$L_P$}池化是一种生物启发的池化过程，以复杂细胞为模型。对其进行了理论分析，结果表明\emph{$L_P$}池化比最大池化具有更好的泛化能力。\emph{$L_P$}池化可以表示为
\begin{equation}
	y_{i,j,k} = [\sum_{(m,n)\in R_{i,j}}(a_{m,n,k})^p]^{\frac{1}{p}}
\end{equation}
其中，$y_{i,j,k}$是第k个特征图在$(i, j)$位置的池化操作的输出，$a_{m,n,k}$是第k个特征图的$R_{i,j}$池化区域在$(m, n)$处的特征值。特别的，当$p = 1$时，\emph{$L_p$}等于平均池化，当$p =\infty$时，，\emph{$L_p$}等于最大池化。
\subsubsection{混合池化(Mixed Pooling)}
Yu等人受random Dropout和DropConnect的启发，提出了一种混合池化方法，即最大池化和平均池化相结合。混合池功能可以表述为如下:
\begin{equation}
	y_{i,j,k}=\lambda\max_{(m,n)\in R_{i,j}}a_{m,n,k}+(1-\lambda)\frac{1}{|R_{i,j}|}\sum_{(m,n)\in R_{i,j}}a_{m,n,k}
\end{equation}
其中$\lambda$是一个0或1的随机值，表示使用平均池或Max池的选择。在正向传播过程中，$\lambda$被记录下来并用于反向传播操作。实验表明，混合池化能更好地解决过拟合问题，其性能优于最大池化和平均池化。
\subsubsection{随机池化(Stochastic Pooling)}
随机池化是一种受辍学启发的池化方法。随机池不像最大池那样在每个池域内选取最大值，而是根据多项式分布随机选取激活值，这保证了特征值的非最大激活值也可以被利用。具体来说，随机池首先通过归一化区域内的激活来计算每个区域$R_j$的概率\emph{p}，即$p_i=a_i/\sum_{k\in R_j}(a_k)$。随后得到概率分布$P(p1，…，p|R_j|)$，我们可以从基于\emph{p}的多项式分布中取样，在区域内选择一个位置\emph{l}，然后设置集合激活为$y_j=a_l$，其中$\textit{l}\sim P(p1，…，p|R_j|)$。与最大池化相比，随机池化可以避免由于随机成分而产生的过拟合。
\subsubsection{频谱池化(Spectral Pooling)}
频谱池化通过在频域裁剪输入表示来进行维数缩减。给定一个输入特征图$x\in R^{mxm}$,假设所需的输出特性图的尺寸是$h×w$,频谱池化首先计算输入特征图的离散傅里叶变换(DFT),然后裁剪通过维持频率表示只有中央h×w频率的子矩阵,最后利用反转DFT将近似值映射回空间域。与最大池化相比，频谱池化的线性低通滤波操作可以在相同的输出维数下保留更多的信息。同时，它也不受其他池化方法输出图维数急剧下降的影响。更重要的是，频谱池化的过程是通过矩阵截断实现的，这使得它能够在使用FFT卷积核的CNN中实现，且计算成本很小。
\subsubsection{空间金字塔池化(Spatial Pyramid Pooling)}
空间金字塔池化(SPP)是由He等人引入的。SPP的主要优点是，它可以生成固定长度的表示，而不管输入大小。SPP池化将特征图输入到与图像大小成比例的局部空间箱中，从而得到固定的箱数量。这与之前的深度网络中的滑动窗口池不同，滑动窗口的数量取决于输入的大小。通过将最后一个池化层替换为SPP，他们提出了一个新的SPP网络，可以处理不同大小的图像。
\subsubsection{多尺度无条理池化(Multi-scale Orderless Pooling)}
Gong等人利用多尺度无序池(MOP)来改善神经网络的不变性，同时不降低其判别能力。他们提取了整个图像和多个尺度的局部块的深度激活特征。整个图像的激活与之前的CNN相同，目的是捕获全局空间布局信息。通过VLAD编码对局部块的激活进行聚合，目的是获取更多局部的、细粒度的图像细节，并增强图像的不变性。通过连接全局激活和局部块激活的VLAD特征，得到新的图像表示。
\subsection{激活函数}
适当的激活函数可以显著提高CNN对某一任务的性能。在本节中，我们将介绍最近在CNN中使用的激活函数
\subsubsection{ReLU}
Rectified linear unit (ReLU)是最引人注目的非饱和活化函数之一。ReLU激活函数定义为:
\begin{equation}
	a_{i,j,k}=\max(z_{i,j,k},0)
\end{equation}
其中$z_{i,j,k}$是激活函数在第k个通道的$(i,j)$处的输入。ReLU是一个分段线性函数，将负的部分修剪为零，保留正的部分(见\figref{fig:relu}(a))。ReLU的简单max(·)运算使得它的计算速度比sigmoid或tanh激活函数快得多，而且它还能诱导隐藏单元的稀疏性，使网络易于获得稀疏表示。研究表明，即使不需要预先训练，使用ReLU也可以有效地训练深度网络。尽管ReLU在0处的不连续性可能会损害反向传播的性能，但许多研究表明，ReLU比sigmoid和tanh激活函数更有效。
\subsubsection{Leaky ReLU}
ReLU单元的一个潜在缺点是，当该单元不活动时，它的梯度为零。这可能会导致最初不活动的单位永远不会活动，因为基于梯度的优化不会调整它们的权重。此外，由于恒定的零梯度，它可能会减慢训练过程。为了缓解这个问题，Mass等人引入了Leaky ReLU (LReLU)，定义为:
\begin{equation}\label{formula10}
	a_{i,j,k}=\max(z_{i,j,k},0)+\lambda\min(z_{i,j,k},0)
\end{equation}
其中λ是(0,1)范围内的预定义参数。与ReLU相比，Leaky ReLU压缩负的部分，而不是将其映射到常量零，这使得它给一个小的非零梯度，当这个单元不活动。
\subsubsection{Parametric ReLU}
与其在leleaky ReLU中使用预定义的参数，例如\equref{formula10}中的$\lambda$， He等人提出了参数整流线性单元(PReLU)，它自适应地学习整流器的参数以提高精度。在数学上，PReLU函数定义为:
\begin{equation}
	a_{i,j,k}=\max(z_{i,j,k},0)+\lambda_k\min(z_{i,j,k},0)
\end{equation}
其中$\lambda_k$是第k个通道的学习参数。由于PReLU只引入非常少量的额外参数，额外参数的数量与整个网络的通道数量相同，因此不存在额外的过拟合风险，额外的计算代价可以忽略。它也可以通过反向传播的方法同时与其他参数进行训练。
\subsubsection{Randomized ReLU}
Leaky ReLU的另一种变体是随机泄漏整流线性单元(RReLU)。在RReLU中，负部分的参数在训练时从均匀分布中随机采样，然后在测试时固定(见\figref{fig:relu}(c))。形式上，RReLU函数定义为:
\begin{equation}
	a^{(n)}_{i,j,k}=\max(z^{(n)}_{i,j,k},0)+\lambda^{(n)}_k\min(z^{(n)}_{i,j,k},0)
\end{equation}
其中$z^{(n)}_{i,j,k}$是第n个例子的第k个通道在$(i, j)$位置激活函数的输入，$\lambda^{(n)_k}$为对应的采样参数，$a^{(n)}_{i,j,k}$为对应的输出。由于它的随机特性，可以减少过拟合。Xu等人也评价了ReLU、LReLU、PReLU和RReLU在标准图像分类任务中的作用，并得出结论:在校正激活单元中加入负部分的非零斜率可以持续提高性能。
\subsubsection{ELU}
Clevert等人引入了指数线性单元(ELU)，使深度神经网络的学习速度更快，并导致更高的分类精度。与ReLU、LReLU、PReLU和RReLU一样，ELU通过将正数部分设置为辨别部分来避免梯度渐变消失问题。与ReLU相比，ELU有利于快速学习的负数部分。相对于LReLU、PReLU、RReLU也有不饱和的负数部分，ELU采用饱和函数作为负数部分。由于饱和函数在失活时将减少单元的变化，使ELU对噪声更具有鲁棒性。ELU的函数定义为:
\begin{equation}
	a_{i,j,k}=\max(z_{i,j,k},0)+\min(\lambda(e^{z_{i,j,k}}-1),0)
\end{equation}
其中$\lambda$是一个预定义的参数，以控制ELU饱和负输入的值。

\subsubsection{Maxout}
Maxout是一个可选非线性函数，它在每个空间位置上具有多个通道的最大响应。maxout函数定义为$a_{i,j,k}=\max_{k\in[i,K]}z_{i,j,k}$，其中在$z_{i,j,k}$是特征图的第k个通道。值得注意的是，maxout享有ReLU的所有好处，因为ReLU实际上是maxout的一种特殊情况，例如，$\max(w^T_1x+b_1,w^T_2x+b_2)$，其中w1是一个零向量，b1是零。此外，maxout特别适合于Dropout的训练
\subsubsection{Probout}
Springenberg等人提出了一种名为probout的maxout的概率变体。它们用概率抽样程序代替了maxout中的最大运算。具体来说，他们首先定义每个k线性单位的概率为:$p_i=e^{\lambda z_i}/\sum^k_{i=1}e^{\lambda z_j}$，其中$\lambda$是控制分布方差的超参数。然后，他们根据多项分布从k个单位中选出一个$\{p1，…， pk\}$，并设置激活值为所选单位的值。为了与dropout相结合，他们实际上重新定义了概率:
\begin{equation}
	\hat{p}_0=0.5,\hat{p}_i=e^{\lambda z_j}/(2.\sum^k_{j=1}e^{\lambda z_j})
\end{equation}
然后将激活函数采样为
\begin{eqnarray}a_i=
\begin{cases}
	0,&\text{if i = 0} \\
	z_i,&\text{else}
\end{cases}
\end{eqnarray}
其中$i\sim$多项式\{$\hat{p}_0,...,\hat{p}_k$\}。Probout可以在保持最大输出单元的理想性质和改善其不变性之间取得平衡。然而，在测试过程中，由于附加的概率计算，probout的计算成本比maxout高。
\subsection{损失函数}
为特定的任务选择适当的损失函数是很重要的。在本节中我们介绍了四个具有代表性的损失函数:Hinge loss, Softmax loss, Contrastive loss, Triplet loss。
\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{6.png}
	\end{overpic}
	\caption{四种激活函数
	}\label{fig:relu}
\end{figure}
\subsubsection{Hinge loss}
Hinge loss通常用于训练大幅度分类器，如支持向量机(SVM)。多类支持向量机的hinge loss定义在\eqref{loss}中，其中w是分类器的权重向量，$y^{(i)}\in[1，. . .， K]$表示它在K个类中正确的类标签。
\begin{equation}\label{loss}
	L_{hinge}=\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^K[\max(0,1-\delta(y^{(i)},j)w^Tx_i)]^p
\end{equation}
其中$\delta(y^{(i)},j)=1 如果 y^{(i)}=j,否则\delta(y^{(i)},j)=1$，注意如果$p=1$等式\eqref{loss}为Hinge-Loss(L1-Loss)，如果$p=2$,则为Hinge-Loss(L2-Loss)。L2-Loss与L1-Loss比较是可微的，并且对违反边际的点施加更大的损失。有人研究并比较了softmax和L2-SVMs在深度网络中的性能。MNIST的结果表明L2-SVM优于softmax。
\subsubsection{Softmax Loss}

Softmax Loss是一种常用的损失函数，实质上是多项logistic loss和Softmax的组合。给定一个训练集$\{(x^{(i)}, y^{(i)});i\in1，. . .,N,y^{(i)}\in1，. . ., K\}$，其中$x^{(i)}$为第i个输入图像块,$y^{(i)}$为其K个类中的目标类标签。对第i个输入的第j类的预测用softmax函数进行变换:$p^{(i)}_j=e^{z_j^{(i)}}/\sum_{l=1}^Ke^{z_l^{(i)}}$，其中$z_j^{(i)}$通常是密连接层的激活，因此$z_j^{(i)}$可以写成$z_j^{(i)}=w^T_ja^{(i)}+b_j$。Softmax将预测转化为非负值，并将其归一化，得到类的概率分布。这种概率预测用于计算多项logistic loss，即softmax loss，如下所示:
\begin{equation}
	L_{softmax}=-\frac{1}{N}[\sum_{i=1}^N\sum_{j=1}^K\{y^{(i)}=j\}logp_j^{(i)}]
\end{equation}
最近，Liu等人提出了大幅度Softmax(L-Softmax)loss，该损失在输入特征向量$a^{(i)}$和权重矩阵的第j列$w_j$之间的角度$\theta_j$引入了一个角幅度。L-Softmax损耗的预测$p^{(i)}_j$定义为:
\begin{equation}
	p^{(i)}_j=\frac{e^{\left\|w_j\right\| \left\|a^{(i)}\right\| \psi(\theta_j)}}{e^{\left\|w_j\right\| \left\|a^{(i)}\right\| \psi(\theta_j)} +\sum_{l\neq j}e^{\left\|w_j\right\| \left\|a^{(i)}\right\| \cos(\theta_l)}}
\end{equation}
\begin{equation}
	\psi(\theta_j)=(-1)^k\cos(m\theta_j)-2k,\theta_j\in[k\pi/m,(k+1)\pi/m]
\end{equation}
其中$k\in[0,m-1]$是一个整数，m控制类之间的幅度。当m = 1时，L-Softmax loss减小到原来的softmax loss。通过调整类间的幅度m，定义一个相对困难的学习目标，可以有效避免过拟合。他们验证了L-Softmax对MNIST、CIFAR-10和CIFAR-100的有效性，并发现L-Softmax的损耗性能优于原softmax。
\subsubsection{Contrastive Loss}
Contrastive Loss通常用于训练Siamese网络，这是一种弱监督方案，用于从标记为匹配或非匹配的成对数据实例中学习相似性度量。已知第i对数据$(x^{(i)}_\alpha，x^{(i)}_\beta)$，设$(z^{(i,l)}_\alpha，z^{(i,l)}_\beta)$表示第l层$(l\in[1，···，l])$对应的输出对。在一篇论文中它们将图像对通过两个相同的CNN，并将最后一层的特征向量提供给cost模块。他们训练样本使用的对比损失函数为:
\begin{equation}
	L_{contrastive}=\frac{1}{2N}\sum_{i=1}^N(y)d^{i,L}+(1-y)max(m-d^(i,L),0)
\end{equation}
其中$d(i,L)=\left\|z_\alpha^{i,L}-z^{i,L}_\beta\right\|_2^2$，m是影响非匹配对的边界参数。如果$(x x^{(i)}_\alpha，x^{(i)}_\beta)$是一个匹配对，则y = 1。否则，y = 0。Lin等人发现，当对所有对进行微调时，这种单一的边际损失函数会导致检索结果的急剧下降。同时，仅对非匹配对进行微调时，性能得到了较好的保留。这表明丢失是由loss函数中对匹配对的处理造成的。虽然仅对非匹配对的召回率是稳定的，但对匹配对的处理是召回率下降的主要原因。为了解决这一问题，他们提出了一个双幅度损失函数，该函数增加了另一个幅度参数来影响匹配对。不是计算最后一层的损失，而是为每一层l定义它们的对比损失，并同时执行单个层的损失的反向传播。它被定义为:
\begin{equation}
\begin{split}
	L_{contrastive}=\frac{1}{2N}\sum_{i=1}^N\sum_{i=1}^L(y)max(d^{i,L}-m_{1,0})\\
	+(1-y)max(m_2-d^{(i,L)},0)
\end{split}
\end{equation}
在实践中，他们发现这两个边缘参数可以设置为相等$(m1= m2= m)$，可以通过采样匹配和非匹配图像对的分布来学习。
\subsubsection{Triplet Loss}
Triplet Loss考虑每个损失函数的三个实例。三元组单元$(x^{(i)}_a,x^{(i)}_p, x^{(i)}_n)$通常包含一个锚实例$x_a^{(i)}$以及来自$x_a^{(i)}$同一类正向实例$x_p^{(i)}$和一个反向实例$x_n^{(i)}$。让$(z_a^{(i)},z_p^{(i)},z_n^{(i)})$表示为三元体单元的特征,Triplet Loss被定义为:
\begin{equation}
	L_{triplet}=\frac{1}{N}\sum_{i=1}^N\max\{d_{(a,p)}^{(i)}-d_{(a,n)}^{(i)}+m,0\}
\end{equation}
其中$d_{(a,p)}^{(i)}=\left\|z_a^{(i)}-z_p^{(i)}\right\|_2^2 = \left\|z_a^{(i)}-z_n^{(i)}\right\|_2^2$。Triplet Loss的目的是使锚点与正向之间的距离最小，而使负向与锚点之间的距离最大。

但在某些特殊情况下，随机选取的锚点样本可能会判断错误。例如，当$d_{(n,p)}^{(i)}<d_{(a,p)}^{(i)}< d_{(a,n)}^{(i)}$时，三元组损失可能仍然为零。因此，在反向传播过程中，将忽略三元组单元。Liu等人提出耦合簇损失(Coupling Clusters, CC)来解决这一问题。耦合簇损失函数是在正集和负集上定义的，而不是使用三元组单元。将随机选取的锚点替换为聚类中心，使正集的样本聚在一起，而负集的样本相对远离，比原来的三元组损失更可靠。耦合簇损失函数定义为:
\begin{equation}
	L_{cc}=\frac{1}{N^p}\sum_{i=1}^{N^p}\frac{1}{2}\max\{ \left\|z^{(i)}_p-c_p\right\|_2^2
	-\left\|z^{(*)}_n-c_p\right\|_2^2 +m,0\}
\end{equation}
其中$N^p$是每个集合的样本数，$z^{(*)}_n$是$x^{(*)}_n$的特征表示，$x^{(*)}_n$是离估计中心点$c_p=(\sum_i^{N^p}z_p^{(i)})/N^p$最近的负样本。三元组损失及其变体被广泛应用于各种任务中，包括再识别、验证和图像检索。

\subsubsection{Kullback-Leibler Divergence}

Kullbeck-Leibler Divergence(KLD)是两个概率分布$p(x)$和$q(x)$在同一个离散变量x上差异的非对称度量(见\figref{fig:KLD}(a))。从$q(x)$到$p(x)$的KLD定义为:
\begin{equation}
	D_{KL}(p\|q)=-H(p(x))-E_p[\log q(x)] 
\end{equation}
\begin{equation}
	=\sum_xp(x)\log p(x)-\sum_xp(x)\log q(x)=\sum_xp(x)\log\frac{p(x)}{q(x)}
\end{equation}
其中$H(p(x))$是p(x)的香农熵，$Ep(\log q(x))$是$p(x)$和$q(x)$的交叉熵。

\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{7.png}
	\end{overpic}
	\caption{KLD
	}\label{fig:KLD}
\end{figure}

KLD被广泛用于各种自动编码器(AEs)的目标函数中作为信息损失的度量。自动编码器的著名变种包括稀疏自动编码器，去噪声自动编码器和变分自动编码器(VAE)。VAE通过贝叶斯推理解释潜在表征。它由两部分组成:编码器将数据样本x压缩为潜在表征$z \sim q_\phi(z|x)$;以及一个解码器，该解码器将这样的表示映射回尽可能接近输入的数据空间$\hat{x}\sim p_\theta(x|z)$。其中$\phi$为编码器参数，$\theta$为解码器参数。在一篇论文中，VAEs试图最大化$\log p(x|\phi,\theta)$的对数似然的变分下界:
\begin{equation}
	L_{vae}=E_{z\sim q_\phi(z|x)}[\log p_\theta(x|z)]-D_{KL}(q_\phi(z|x)\|p(z))
\end{equation}
其中第一项是重建成本，KLD项强制对分布$q\phi(z|x)$施加先验$p(z)$。通常$p(z)$是标准正态分布，离散分布，或一些具有几何解释的分布。继最初的VAE之后，许多变种已经被提出。条件VAE(CVAE)从条件分布中使用$\hat{x}\sim p_\theta(x|y,z)$生成样本。去噪VAE(DVAE)从损坏的输入$\hat{x}$中恢复原始输入x。

Jensen-Shannon Divergence (JSD)是KLD的对称形式。它衡量了$p(x)$和$q(x)$之间的相似性:
\begin{equation}
\begin{split}
	D_{JS}(p||q)=\frac{1}{2}D_{KL}\left(p(x)||\frac{p(x)+q(x)}{2}\right)+\\
	\frac{1}{2}D_{KL}\left(q(x)||\frac{p(x)+q(x)}{2}\right)
\end{split}
\end{equation}

通过最小化JSD，我们可以使$p(x)$和$q(x)$两个分布尽可能接近。JSD已成功应用于生成对抗网络(GANs)。与直接建模x和z之间关系的VAEs不同，GANs被明确地设置为优化生成任务。GANs的目标是找到能在真实数据和生成数据之间给出最佳区分的鉴别器D，同时鼓励生成器G拟合真实数据分布。鉴别器D和生成器G之间的最小-最大博弈被以下目标函数形式化:
\begin{equation}\label{gan}
	\begin{split}
	\substack{\min \\G}\substack{\max \\D}\mathcal{L}_{gan}(D,G)=E_{x\sim p(x)}[\log D(x)]+ \\
	E_{z\sim q(z)}[\log(1-D(G(z)))]
\end{split}
\end{equation}
最初的GAN论文表明，对于固定生成器$G^*$，我们有最优鉴别器$D^∗G(x) =\frac{p(x)}{p(x)+q(x)}$。那么方程\eqref{gan}等价于使JSD在$p(x)$和$q(x)$之间最小化。如果G和D有足够的容量，分布$q(x)$收敛于$p(x)$。与条件式VAE一样，条件式GAN(CGAN)也接收额外的信息y作为输入，以生成约束y的样本。在实践中，众所周知，GANs在训练时是不稳定的。
\subsection{Regularization}
在深度神经网络中，过拟合是一个不可忽视的问题，通过正则化可以有效地降低过拟合。在下面的小节中，我们将介绍一些有效的正则化技术:$\ell_p$-norm，Dropout和DropConnect。
\subsubsection{$\ell_p$-norm Regularization}
正则化通过增加额外的项来修改目标函数，以减少模型的复杂性。形式上，如果损失函数为$\mathcal{L}(\theta,x,y)$，则正则化损失为:
\begin{equation}
	E(\theta,x,y)=\mathcal{L}(\theta,x,y)+\lambda R(\theta)
\end{equation}
其中$R(\theta)$是正则化项，$\lambda$是正则化强度。

$\ell_p$-norm正则化函数通常采用$R(\theta) =\sum_j||\theta_j||_p^p$，当$P\geq1$时，$\ell_p$-norm是凸的，使优化更容易，使该函数具有吸引力。对于p=2， $\ell_p$-norm正则化通常被称为权重衰减。$\ell_p$-norm的一个更有原则的替代方案是Tikhonov正则化，它奖励对输入噪声的不变性。当p<1时，$\ell_p$-norm正则化更多地利用了权值的稀疏性，但对非凸函数有一定的作用。
\subsubsection{Dropout}
Dropout首先是由Hinton等人引入的，已经被证明在减少过拟合方面非常有效。在他们的论文中，他们对完全连接的层应用Dropout。Dropout的输出是$y = r∗a(W^T\bm{x})$，其中$\bm{x} = [x_1, x_2，…,x_n]^T$是全连接层的输入，$W\in R^{n\times d}$是一个权值矩阵，$\bm{r}$是一个大小为d的二进制向量，其元素是由参数为p的伯努利分布独立得出的，即$r_i\sim Bernoulli(p)$。Dropout可以防止网络过于依赖任何一个神经元(或任何一个小的神经元组合)，并且可以迫使网络在缺乏特定信息的情况下保持准确。已经提出了几种改进Dropout的方法。Wang等人提出了一种快速Dropout方法，该方法可以通过采样或积分高斯近似来进行快速Dropout训练。Ba等人[91]提出了一种自适应Dropout方法，其中每个隐藏变量的Dropout概率使用与深度网络共享参数的二进制网络来计算。在论文中，他们发现在1 × 1卷积层之前使用标准的Dropout一般会增加训练时间，但不能防止过拟合。因此，他们提出了一个名为SpatialDropout的新Dropout方法，它将Dropout值扩展到整个特征图。这种新的Dropout方法尤其适用于训练数据量较小的情况。
\subsubsection{DropConnect}
DropConnect将Dropout的理念更进一步。DropConnect不是将神经元的输出随机设置为零，而是将权重矩阵$\bm{W}$的元素随机设置为零。DropConnect的输出由$y=a((R*W)x)$给出，其中$R_{i,j}\sim Bernoulli(p)$。此外，在训练过程中，偏差也被掩盖了。\figref{fig:drop}说明了No-Drop，Dropout和DropConnect网络之间的差异。
\begin{figure}[t!]
	\begin{overpic}[width=\columnwidth]{8.png}
	\end{overpic}
	\caption{三种网络的描述
	}\label{fig:drop}
\end{figure}
\subsection{最优化}
在本小节中，我们将讨论优化CNNs的一些关键技术。
\subsubsection{数据增强}
深层的CNNs特别依赖于大量训练数据的可用性。与CNNs中涉及的参数数量相比，一个缓解数据相对稀缺的优雅解决方案是数据增强。数据增强包括将可用数据转换为新数据而不改变其性质。流行的增强方法包括简单的几何变换，如采样、镜像、旋转、移动和各种光度变换。Paulin等人提出了一种贪婪策略，从一组候选变换中选择最佳变换。然而，他们的策略涉及大量的模型再训练步骤，当候选转换的数量很大时，这在计算上是昂贵的。Hauberg等人提出了一种优雅的方法，通过随机生成微分态来进行数据增强。Xie等人和Xu等人提供了从互联网收集图像的额外方法，以改善细粒度识别任务中的学习。
\subsubsection{权重初始化}
深层的CNN具有大量的参数，其损失函数是非凸的，这使得它很难训练。为了在训练中实现快速收敛，避免消失梯度问题，适当的网络初始化是最重要的先决条件之一。偏移参数可以初始化为零，而权值参数应谨慎初始化，以打破同一层隐藏单元之间的对称性。如果网络没有正确初始化，例如，每一层将其输入缩放为k，最终的输出将缩放原始输入为$k^L$，其中L是层数。在这种情况下，$k > 1$的值会导致输出层的值非常大，而$k < 1$的值会导致输出值递减，并且呈梯度化。Krizhevsky等人将其网络的权值从标准差为0.01的零均值高斯分布初始化，并将第2、4、5个卷积层以及所有全连接层的偏差项设为常数1。另一种著名的随机初始化方法是``Xavier''。他们从一个均值为零、方差为$\frac{2}{n_{in}+n_{out}}$的高斯分布中选择权值，其中$n_{in}$为输入该分布的神经元数量，而$n_{out}$为输出结果的神经元数量。因此``Xavier''可以根据输入输出神经元的数量自动确定初始化规模，并通过多层将信号保持在一个合理的值范围内。它在Caffe中的一个变体仅使用了$n_{in}$-only，这使得它更容易实现。``Xavier''初始化方法后来被扩展，以考虑校正的非线性，其中他们推导了一个鲁棒的初始化方法，特别考虑ReLU的非线性。他们的方法允许训练极深的模型使其收敛，而``Xavier''方法却不能。

Saxe等人独立地表明，正交矩阵初始化对于线性网络比高斯初始化更有效，对于非线性网络也同样有效。Mishkin等人将其扩展为一个迭代过程。具体来说，它提出了一种层序单位方差处理方案，可以将其视为只对第一个小批处理执行的标准正交初始化与批标准化相结合(见3.6.4节)。它类似于批量归一化，因为它们都采用了单位方差归一化处理。不同的是，它使用正交归一化来初始化权重，这有助于有效地去相关层活动。这种初始化技术被应用了，而且性能显著提高。

\subsubsection{随机梯度下降}
反向传播算法是使用梯度下降更新参数的标准训练方法。许多梯度下降优化算法已经被提出。标准梯度下降算法将目标$\mathcal{L}(\theta)$的参数$\theta$更新为$\theta_{t+1} = \theta_t-\eta\nabla_\theta\emph{E}[\mathcal{L}(\theta_t)]$，其中$\emph{E}[\mathcal{L}(\theta_t)]$是$\mathcal{L}(\theta_t)$在整个训练集上的期望，$\eta$是学习率。随机梯度下降(SGD)不是计算$\mathcal{L}(\theta_t)$，而是从训练集中随机选取的单个样例$(x^{(t)},y^{(t)})$估计梯度。
\begin{equation}
	\theta_{t+1} = \theta_t-\eta_t\nabla_\theta\mathcal{L}(\theta_t;x^{(t)},y^{(t)})
\end{equation}

在实践中，SGD中的每个参数更新都是根据一个小批处理计算的，而不是单个示例。这有助于减少参数更新中的方差，并能促使更稳定的收敛。收敛速度由学习率$\eta_t$控制。然而，小批量SGD并不能保证良好的收敛性，仍然存在一些需要解决的挑战。首先，选择一个合适的学习率并不容易。一种常见的方法是使用一个恒定的学习率，在初始阶段给出稳定的收敛，然后随着收敛速度的减慢而降低学习率。另外，学习速率计划被提出用于在训练过程中调整学习率。为了使当前梯度更新依赖于历史批次和加速训练，动量被提出用于在相关方向累加速度矢量。经典的动量更新公式是这样定义的:
\begin{equation}
	v_{t+1} = \gamma v_t-\eta_t\nabla_\theta\mathcal{L}(\theta_t;x^{(t)},y^{(t)})
\end{equation}
\begin{equation}
	\theta_{t+1} = \theta_t+v_{t+1}
\end{equation}
其中$v_{t+1}$是当前的速度矢量，$\gamma$是动量项，通常设置为0.9。Nesterov动量是在梯度下降优化中使用动量的另一种。
\begin{equation}
	v_{t+1} = \gamma v_t-\eta_t\nabla_\theta\mathcal{L}(\theta_t+\gamma v_t;x^{(t)},y^{(t)})
\end{equation}
与经典动量先计算当前梯度，然后向更新累加梯度的方向移动相比，Nesterov动量先向先前累积梯度$\gamma v_t$的方向移动，计算梯度，然后进行梯度更新。这种预期的更新可以防止优化进行得太快，从而获得更好的性能。

并行SGD方法改进了SGD，使之适合于并行、大规模机器学习。与标准(同步)SGD不同的是，在标准(同步)SGD中，如果一台机器太慢，训练就会被延迟，这个并行化方法使用异步机制，因此除了最慢的机器上的优化外，没有其他优化会被延迟。Jeffrey Dean等人使用另一种称为Downpour SGD的异步SGD程序来加快具有多个cpu的集群上的大规模分布式训练进程。也有一些工作是使用多个GPU的异步SGD。Paine等人基本上将异步SGD与gpu结合起来，与在单机上训练相比，可以将训练时间缩短几倍。Zhuang等人也使用多个GPU异步计算梯度并更新全局模型参数，在4个GPU上实现的加速比在单个GPU上的快3.2倍。

注意，SGD方法可能不会促使收敛。当性能停止提升时，训练过程可以终止。对过度训练的一个流行的补救方法是使用早期停止，在训练期间，基于验证集的性能停止优化。为了控制训练过程的持续时间，可以考虑不同的停止标准。例如，训练可能执行固定的epoch数，或直到达到预定义的训练错误。停止策略需要谨慎操作，在提高网络泛化能力和避免过拟合的前提下，适当的停止策略应该让训练过程继续进行。

\subsubsection{批量归一化}
数据归一化通常是数据预处理的第一步。全局数据归一化将所有数据转化为零均值和单位方差。但是，当数据流经深层网络时，输入到内层的分布会发生变化，从而失去网络的学习能力和准确性。Ioffe等人提出了一种有效方法称为批量归一化(Batch Normalization, BN)来部分缓解这一现象。它通过一个归一化步骤来解决所谓的协变量偏移问题，该步骤固定了输入层的均值和方差，其中均值和方差的估计是在每一个小批次后计算的，而不是整个训练集。假设待规格化的层有一个d维输入，即$x = [x_1, x_2,…,x_d]^T$。我们首先将第k维归一化如下:
\begin{equation}
	\hat{x}_k = (x_k-\mu_\mathcal{B})/\sqrt{\delta^2_\mathcal{B} + \epsilon}
\end{equation}
其中$\mu_\mathcal{B}$和$\delta^2_\mathcal{B}$分别为小批量的均值和方差，$\delta$是一个常数值。为了增强表示能力，将归一化输入$\hat{x}_k$进一步转换为:
\begin{equation}
	y_k = BN_{\gamma,\beta}(x_k)=\gamma\hat{x}_k+\beta
\end{equation}
其中$\gamma$和$\beta$是学习参数。与全局数据归一化相比，批量归一化具有许多优点。首先，它减少了内部协变量偏移。其次，BN减少了梯度对参数尺度或其初始值的依赖，从而对网络中的梯度流动产生有益的影响。这使得使用更高的学习率，而没有出现分歧的风险。此外，BN正则化了模型，从而减少了Dropout的需要。最后，BN使使用饱和非线性激活函数而不陷入饱和模型成为可能。
\subsubsection{快捷连接}
如上所述，通过规一初始化和BN可以缓解深度神经网络的消失梯度问题。尽管这些方法成功地防止了深度神经网络的过拟合，但它们也给网络的优化带来了困难，导致其性能比浅层网络差。这种深层神经网络所遭受的优化问题被视为退化问题。

受长短期记忆(LSTM)网络的启发，该网络使用门函数来确定有多少神经元的激活值要转换或只是通过。Srivastava等人提出了能够以几乎任意深度优化网络的高速网络。其网络的输出是:
\begin{equation}
	x_{l+1} = \phi_{l+1}(x_l,W_H)·\tau_{l+1}(x_l,W_T)+x_l·(1-\tau(x_l,W_T))
\end{equation}
其中$x_l$和$x_{l+1}$对应第$l^{th}$块的输入和输出，$\tau(·)$是变换门，$\phi(·)$通常是一个仿射变换后的非线性激活函数(通常它也可能采取其他形式)。这种门机制迫使该层的输入和输出具有相同的大小，并允许数十或数百层的高速公路网络进行有效训练。不同的输入样例，门的输出有很大的不同，说明网络不仅学习固定的结构，而且会根据特定的样例动态路由数据。


残差网络(ResNets)与LSTM单元中工作的核心思想相同。ResNets中的快捷连接没有门控，未转换的输入直接传播到输出，从而带来更少的参数，而不是为神经元特定的门控使用可学习的权值。ResNets的输出如下所示:
\begin{equation}
	x_{l+1}=x_l+f_{l+1}(x_l,W_F)
\end{equation}
其中$f_l$是权值层，它可以是卷积、BN、ReLU或池化等操作的复合函数。对于残差块，任何较深单元的激活都可以写成较浅单元激活和残差块函数之和。这也意味着梯度可以直接传播到较浅的单位，这使得深度ResNets比原来的映射函数更容易优化，也更有效地训练非常深的网。这与通常的前馈网络形成了对比，后者的梯度本质上是一系列矩阵-向量乘积，随着网络的深入，它们可能会消失。

在最初的ResNets之后，He等人[123]继续使用另一种ResNets预激活变体，在那里他们进行了一系列实验，以证明快捷连接是网络最容易学习的。他们还发现，将BN前置比在加法后使用BN性能要好得多。在他们的比较中，使用BN + ReLU预激活的残差网比之前的ResNets得到更高的精度。Shen等在卷积层的输出中引入一个加权因子，逐步引入可训练层。最新的Inception-v4论文还报告了通过跨Inception模块使用跳跃连接来加速训练和提高性能。原来的ResNets和预激活ResNets非常深，但也非常薄。而Wide ResNets则提出减小深度，增加宽度，在CIF AR-10、CIF AR-100和SVHN上取得了令人印象深刻的结果。然而，他们的说法并没有在Imagenet dataset\footnote{http://www.image-net.org}上的大规模图像分类任务中得到验证。随机深度ResNets随机丢弃层的子集，并通过每个小批量的映射绕过它们。Singh等人将随机深度ResNets和Dropout结合起来，推广了Dropout和具有随机深度的网络，可以将其视为ResNets、Dropout ResNets和随机深度ResNets的集合。ResNets (RiR)论文中的ResNets描述了一种融合经典卷积网络和残差网络的架构，其中RiR的每个块包含残差单元和非残差块。RiR可以知道每个残差块应该使用多少个卷积层。ResNets of ResNets(RoR)是对ResNets架构的修改，该架构建议使用多级快捷连接，而不是先前在ResNets上的工作中的单层快捷连接。DenseNet可以看作是一种将跳跃连接的洞察力发挥到极致的架构，其中一层的输出连接到该模块中所有后续层。在所有的ResNets中，Highway和Inception网络中，我们可以看到一个相当明显的趋势，即使用捷径连接来帮助训练非常深的网络。
\section{CNNs的快速处理}

随着计算机视觉和机器学习任务的不断挑战，深度神经网络的模型变得越来越复杂。这些强大的模型需要更多的数据进行训练，以避免过拟合。同时，大的训练数据也带来了新的挑战，如怎样在可行的时间内训练网络。在这一节中，我们将介绍一些CNNs的快速处理方法。
\subsection{FFT}
Mathieu等人利用FFTs在傅里叶域进行卷积运算。使用基于FFT的方法有许多优点。首先，滤波器的傅里叶变换可以重复使用，因为滤波器与多幅图像在一个小批量进行卷积。其次，输出梯度的傅里叶变换可以在向滤波器和输入图像反向传播梯度时重用。最后，对输入通道的求和可以在傅里叶域中进行，这样每个图像的每个输出通道只需要进行一次傅里叶逆变换。已经开发了一些基于gpu的库来加快训练和测试过程，比如cuDNN和fbfft。然而，使用FFT进行卷积需要额外的内存来存储傅里叶域的特征映射，因为滤波器必须被填充成与输入相同的大小。当步长参数大于1时，这种代价尤其昂贵，这在许多先进的网络中很常见，如一些论文中的早期层。FFT可以实现更快的训练和测试过程，而小尺寸卷积滤波器的日益突出已经成为CNNs(如ResNet和GoogleNet)的重要组成部分，这使得一种专门针对小尺寸滤波器的新方法:Winograd的最小滤波算法。Winograd的想法类似于FFT，在应用逆变换之前，可以在变换空间中跨通道减少Winograd卷积，从而使推理更加有效。
\subsection{结构转换}
低秩矩阵分解在各种情况下被用来改进优化问题。给定一个秩为 $r$的m×n矩阵C，存在一个分解因式$C = AB$，其中A是m×r满列秩矩阵，B是r×n满行秩矩阵。因此，我们可以用A和B来代替C。要将C的参数减少一个分数$p$，必须保证$mr+rn < pmn$，即C的秩应满足$r < pmn/(m + n)$。
通过这种分解，空间复杂度从$O(mn)$降低到$O(r(m + n))$，时间复杂度从$O(mn)$降低到$O(r(m + n))$。为此，Sainath等人将低秩矩阵分解应用于深度CNN的最终权重层，在精度损失较小的情况下，提高了约30-50\%的训练速度。同样，Xue等人在深度CNN的每一层上应用奇异值分解，使模型规模减少71\%，相对精度损失小于1\%。Denton等人和Jderberg等人受深度神经网络参数冗余的启发，独立研究了卷积滤波器中的冗余，并发展了近似来减少所需的计算。Novikov等人推广了低秩的思想，他们将权矩阵视为多维张量，并应用Tensor-Train分解来减少全连接层的参数数量。



{\small
\bibliographystyle{ieee}
\bibliography{Saliency}
}

% \end{CJK*}
\end{document}
